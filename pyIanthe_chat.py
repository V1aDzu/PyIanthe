import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import pyIanthe_config

# --- –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –º–æ–¥–µ–ª—ñ ---
author, model_name = pyIanthe_config.MODEL_ID.split("/")
model_dir = os.path.join(pyIanthe_config.FOLDER_MODELS, author, model_name)
device = "cuda" if torch.cuda.is_available() else "cpu"

# --- –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –º–æ–¥–µ–ª—å —Ç–∞ —Ç–æ–∫–µ–Ω—ñ–∑–∞—Ç–æ—Ä –ª–æ–∫–∞–ª—å–Ω–æ ---
model = AutoModelForCausalLM.from_pretrained(
    model_dir,
    dtype="auto",
    device_map="auto"
).to(device)
tokenizer = AutoTokenizer.from_pretrained(model_dir)

# --- –ö–ª–∞—Å —á–∞—Ç-–±–æ—Ç–∞ ---
class ChatBot:
    def __init__(self, model, tokenizer, device, max_history=10):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device
        self.max_history = max_history  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ—Ö –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω—å —É —ñ—Å—Ç–æ—Ä—ñ—ó
        self.messages = [
            {"role": "system", "content": "–í–∏ ‚Äî –∫–æ—Ä–∏—Å–Ω–∏–π –ø–æ–º—ñ—á–Ω–∏–∫."}
        ]

    def chat(self, user_message: str, max_new_tokens: int = 200,
             temperature: float = 0.7, top_p: float = 0.9):
        # –î–æ–¥–∞—î–º–æ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞
        self.messages.append({"role": "user", "content": user_message})

        # –û–±—Ä—ñ–∑–∞—î–º–æ —ñ—Å—Ç–æ—Ä—ñ—é –¥–æ max_history, –∑–∞–ª–∏—à–∞—é—á–∏ –ø–µ—Ä—à–µ —Å–∏—Å—Ç–µ–º–Ω–µ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è
        if len(self.messages) > self.max_history + 1:
            self.messages = [self.messages[0]] + self.messages[-self.max_history:]

        # –§–æ—Ä–º—É—î–º–æ prompt —á–µ—Ä–µ–∑ —à–∞–±–ª–æ–Ω —á–∞—Ç—É
        prompt = self.tokenizer.apply_chat_template(
            self.messages,
            tokenize=False,
            add_generation_prompt=True
        )

        # –ü–µ—Ä–µ—Ç–≤–æ—Ä—é—î–º–æ —É —Ç–µ–Ω–∑–æ—Ä–∏
        model_inputs = self.tokenizer([prompt], return_tensors="pt").to(self.device)

        # –ì–µ–Ω–µ—Ä—É—î–º–æ –≤—ñ–¥–ø–æ–≤—ñ–¥—å
        output_ids = self.model.generate(
            **model_inputs,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            top_p=top_p,
            do_sample=True,
            pad_token_id=self.tokenizer.eos_token_id
        )[0]

        # –ë–µ—Ä–µ–º–æ –ª–∏—à–µ –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω—É —á–∞—Å—Ç–∏–Ω—É (–≤—ñ–¥–ø–æ–≤—ñ–¥—å –±–æ—Ç–∞)
        generated_ids = output_ids[model_inputs.input_ids.shape[-1]:]
        response = self.tokenizer.decode(generated_ids, skip_special_tokens=True).strip()

        # –î–æ–¥–∞—î–º–æ –≤—ñ–¥–ø–æ–≤—ñ–¥—å –±–æ—Ç–∞ –≤ —ñ—Å—Ç–æ—Ä—ñ—é
        self.messages.append({"role": "assistant", "content": response})

        return response

    def reset(self):
        self.messages = [
            {"role": "system", "content": "–í–∏ ‚Äî –∫–æ—Ä–∏—Å–Ω–∏–π –ø–æ–º—ñ—á–Ω–∏–∫."}
        ]

# --- –Ü–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∏–π —á–∞—Ç ---
def interactive_chat():
    bot = ChatBot(model, tokenizer, device, max_history=10)
    print("ü§ñ PyIanthe (Qwen) ‚Äì —á–∞—Ç. –í–≤–µ–¥—ñ—Ç—å /exit —â–æ–± –≤–∏–π—Ç–∏.")
    while True:
        try:
            user_input = input("–í–∏: ").strip()
            if not user_input:
                continue
            if user_input.lower() in ["/exit", "/quit"]:
                break
            result = bot.chat(user_input, max_new_tokens=200, temperature=0.7, top_p=0.9)
            print("–ë–æ—Ç:", result)
        except KeyboardInterrupt:
            print("\n–í–∏—Ö—ñ–¥ –∑ —á–∞—Ç—É...")
            break
        except Exception as e:
            print("–ü–æ–º–∏–ª–∫–∞:", e)

if __name__ == "__main__":
    interactive_chat()
